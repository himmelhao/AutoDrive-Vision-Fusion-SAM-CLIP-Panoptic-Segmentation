{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7c0041e",
   "metadata": {},
   "source": [
    "# Automatically generating object masks with MobileSAM"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0b71431",
   "metadata": {},
   "source": [
    "## Environment Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6964c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/openai/CLIP.git\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fd2bc687",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_anns(anns):\n",
    "    if len(anns) == 0:\n",
    "        return\n",
    "    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)\n",
    "    ax = plt.gca()\n",
    "    ax.set_autoscale_on(False)\n",
    "\n",
    "    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))\n",
    "    img[:,:,3] = 0\n",
    "    for ann in sorted_anns:\n",
    "        m = ann['segmentation']\n",
    "        color_mask = np.concatenate([np.random.random(3), [0.35]])\n",
    "        img[m] = color_mask\n",
    "    ax.imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbc7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): TinyViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (seq): Sequential(\n",
       "        (0): Conv2d_BN(\n",
       "          (c): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Conv2d_BN(\n",
       "          (c): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ConvLayer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x MBConv(\n",
       "            (conv1): Conv2d_BN(\n",
       "              (c): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act1): GELU(approximate='none')\n",
       "            (conv2): Conv2d_BN(\n",
       "              (c): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
       "              (bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act2): GELU(approximate='none')\n",
       "            (conv3): Conv2d_BN(\n",
       "              (c): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (act3): GELU(approximate='none')\n",
       "            (drop_path): Identity()\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (act): GELU(approximate='none')\n",
       "          (conv1): Conv2d_BN(\n",
       "            (c): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d_BN(\n",
       "            (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d_BN(\n",
       "            (c): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): BasicLayer(\n",
       "        dim=128, input_resolution=(128, 128), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x TinyViTBlock(\n",
       "            dim=128, input_resolution=(128, 128), num_heads=4, window_size=7, mlp_ratio=4.0\n",
       "            (drop_path): Identity()\n",
       "            (attn): Attention(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "              (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=128, out_features=512, bias=True)\n",
       "              (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (local_conv): Conv2d_BN(\n",
       "              (c): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
       "              (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (act): GELU(approximate='none')\n",
       "          (conv1): Conv2d_BN(\n",
       "            (c): Conv2d(128, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d_BN(\n",
       "            (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=160, bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d_BN(\n",
       "            (c): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): BasicLayer(\n",
       "        dim=160, input_resolution=(64, 64), depth=6\n",
       "        (blocks): ModuleList(\n",
       "          (0-5): 6 x TinyViTBlock(\n",
       "            dim=160, input_resolution=(64, 64), num_heads=5, window_size=14, mlp_ratio=4.0\n",
       "            (drop_path): Identity()\n",
       "            (attn): Attention(\n",
       "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (qkv): Linear(in_features=160, out_features=480, bias=True)\n",
       "              (proj): Linear(in_features=160, out_features=160, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=160, out_features=640, bias=True)\n",
       "              (fc2): Linear(in_features=640, out_features=160, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (local_conv): Conv2d_BN(\n",
       "              (c): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=160, bias=False)\n",
       "              (bn): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (downsample): PatchMerging(\n",
       "          (act): GELU(approximate='none')\n",
       "          (conv1): Conv2d_BN(\n",
       "            (c): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv2): Conv2d_BN(\n",
       "            (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (conv3): Conv2d_BN(\n",
       "            (c): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): BasicLayer(\n",
       "        dim=320, input_resolution=(64, 64), depth=2\n",
       "        (blocks): ModuleList(\n",
       "          (0-1): 2 x TinyViTBlock(\n",
       "            dim=320, input_resolution=(64, 64), num_heads=10, window_size=7, mlp_ratio=4.0\n",
       "            (drop_path): Identity()\n",
       "            (attn): Attention(\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (qkv): Linear(in_features=320, out_features=960, bias=True)\n",
       "              (proj): Linear(in_features=320, out_features=320, bias=True)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=320, out_features=1280, bias=True)\n",
       "              (fc2): Linear(in_features=1280, out_features=320, bias=True)\n",
       "              (act): GELU(approximate='none')\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (local_conv): Conv2d_BN(\n",
       "              (c): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=320, bias=False)\n",
       "              (bn): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (norm_head): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n",
       "    (head): Linear(in_features=320, out_features=1000, bias=True)\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(320, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"MobileSAM\")\n",
    "from mobile_sam import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam_checkpoint = \"MobileSAM/weights/mobile_sam.pt\"\n",
    "model_type = \"vit_t\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "sam.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d197131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0263b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "def generate_text_embeddings(classnames, templates, model):\n",
    "    with torch.no_grad():\n",
    "        class_embeddings_list = []\n",
    "        for classname in classnames:\n",
    "            texts = [template.format(classname) for template in templates] #format with class\n",
    "            texts = clip.tokenize(texts).to(device) #tokenize\n",
    "            class_embedding = model.encode_text(texts) #embed with text encoder\n",
    "            class_embeddings_list.append(class_embedding)\n",
    "        class_embeddings = torch.stack(class_embeddings_list, dim=1).to(device)\n",
    "    return class_embeddings\n",
    "\n",
    "def create_clip(classes=city_classes):\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/16\", device=device)\n",
    "    clip_model.eval()\n",
    "    text_features = generate_text_embeddings(classes, ['a clean origami {}.'], clip_model)#['a rendering of a weird {}.'], model)\n",
    "    return clip_model, preprocess, text_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51f6346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import visualize\n",
    "\n",
    "def predict(image_path, mask_generator, clip_model, clip_preprocess, text_features):\n",
    "    image = np.array(Image.open(image_path))\n",
    "\n",
    "    torch.cuda.empty_cache()  # Empty GPU memory\n",
    "    outputs = mask_generator.generate(image)\n",
    "\n",
    "    clip_model.eval()\n",
    "\n",
    "    boxes = []\n",
    "    masks = []\n",
    "    class_ids = []\n",
    "    scores = []\n",
    "\n",
    "    for output in outputs:\n",
    "        mask = output[\"segmentation\"]\n",
    "        masked_image = image.copy()\n",
    "        ind = np.where(mask > 0)\n",
    "        masked_image[mask == 0] = 0\n",
    "        y1, x1, y2, x2 = min(ind[0]), min(ind[1]), max(ind[0]), max(ind[1])\n",
    "        masked_image = Image.fromarray(masked_image[y1:y2+1, x1:x2+1])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            masked_image = clip_preprocess(masked_image)\n",
    "            image_features = clip_model.encode_image(masked_image.unsqueeze(0).to(device))\n",
    "\n",
    "            # Pick the top 5 most similar labels for the image\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_features = text_features.squeeze(0)\n",
    "            similarity = (100.0 * image_features.float() @ text_features.float().T).softmax(dim=-1)\n",
    "            score, index = similarity[0].topk(1)\n",
    "\n",
    "        del masked_image  # Release image variables\n",
    "\n",
    "        filtered_class = [\n",
    "            'road', 'sidewalk', 'parking', # 'rail track',\n",
    "            'person', 'rider',\n",
    "            'car', 'truck', 'bus',\n",
    "            #'on rails',\n",
    "            'motorcycle', 'bicycle', #'caravan', 'trailer',\n",
    "            # 'building', 'wall', 'fence',\n",
    "            #'guard rail',\n",
    "            'bridge', 'tunnel',\n",
    "            # 'pole', 'pole group', 'traffic sign', 'traffic light',\n",
    "            'vegetation', 'terrain',\n",
    "            'sky',\n",
    "            'ground', # 'dynamic', 'static'\n",
    "        ]\n",
    "\n",
    "        if city_classes[index.item()] in filtered_class:\n",
    "            boxes.append(convert_xywh_yxyx(output[\"bbox\"]))\n",
    "            masks.append(mask)\n",
    "            scores.append(score.item())\n",
    "            class_ids.append(index.item())\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    masks = np.stack(masks, axis=-1)\n",
    "    class_ids = np.array(class_ids)\n",
    "    scores = np.array(scores)\n",
    "\n",
    "    torch.cuda.empty_cache()  # Empty GPU memory\n",
    "\n",
    "    return image, boxes, masks, class_ids, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df65c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "549c2d2aec05e6a45121638ba126230b4c727bbb16317d1538b4f58d53ef1be1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
